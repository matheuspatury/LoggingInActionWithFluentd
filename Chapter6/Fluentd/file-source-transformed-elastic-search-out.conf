# This configuration for Chapter 6 to illustrate the user of filtering and Elastic Search
<system>
    Log_Level info
</system>

#### begin - tail basic file
<source>
  @type tail
  @id tail
  path ./Chapter6/basic-file.txt
  read_lines_limit 5
  tag basicFile
  pos_file ./Chapter6/basic-file-read.pos_file
  read_from_head true
  <parse>
    @type json
  </parse>
</source>

<source>
  @type dummy
  @id dummy
  tag dummySrc
   auto_increment_key counter
   #dummy {"msg" : "The first computer dates back to Adam and Eve. It was an Apple with limited memory, just one byte. And then everything crashed.", "name" : "{\"firstname\": \"Bart\", \"surname\":\"Fudd\"}", "age": 25}
   dummy {"msg" : "The first computer dates back to Adam and Eve. It was an Apple with limited memory, just one byte. And then everything crashed.", "name" : {"firstname": "Bart", "surname":"Fudd"}, "age": 25}
</source>

<filter *>
  @type stdout
    <inject>
    post source
  </inject>
</filter>

<filter *>
  @type grep
  <regexp>
    key msg
    pattern /computer/
  </regexp>
</filter>

<filter *>
  @type record_transformer
  enable_ruby true
  <record>
    computer ${hostname}
    from  ${record.dig("name", "firstname")}
    msg processed ${record["msg"]}
  </record>
    remove_keys $.name.surname
</filter>

<filter *>
  @type stdout
  <inject>
    post transform
  </inject>
</filter>

#<match other>
#  @type elasticsearch
#  host localhost
#  port 9200
#  logstash_format true
#  reload_on_failure true
#  include_tag_key true
#  tag_key key
#  <buffer>
#    #buffer_type memory
#    flush_interval 15s
#  </buffer>
#</match>

<match *>
  @type stdout
</match>
